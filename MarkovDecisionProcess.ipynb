{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "###### By Rohit Pardasani\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining agent-environment inteface through MDP\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.<br><br>\n",
    "The learner and decision maker is called the agent. <br>\n",
    "The thing it interacts with, comprising everything outside the agent, is called the environment.<br>\n",
    "These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.<br>\n",
    "The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.<br>\n",
    "<img src='fig1.png' width='400'>\n",
    "<img src='fig2.png' width='350'>\n",
    "<img src='fig3.png' width='450'>\n",
    "<img src='fig4.png' width='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics of MDP defined by function $p$\n",
    "<img src='fig5.png' width='400'>\n",
    "<img src='fig6.png' width='400'><br>\n",
    "In a Markov decision process, the probabilities given by $p$ completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $R_t$ depends only on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property. We will assume the Markov property throughout\n",
    "<img src='fig7.png' width='650'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Rewards\n",
    "<b>Reward Hypothesis:</b><br>\n",
    "That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).<br>\n",
    "The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns and Episodes\n",
    "<b>Episodic Tasks:</b>The tasks that have terminal state and thus can be broken into episodes. For example a game that has ends with an outcome win/lose/draw.<br><br>\n",
    "<b>Continuing Tasks:</b>In many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span.<br><br><br>\n",
    "In general, we seek to maximize the expected return, where the return, denoted $G_t$, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\n",
    "<img src='fig8.png' width='300'>\n",
    "Here $T$ is final time step.<br><br><br>\n",
    "The return formulation is problematic for continuing tasks because the final time step would be $T = \\infty$, and the return, which is what we are trying to maximize, could itself easily be infinite. (For example, suppose the agent receives a reward of +1 at each time step.)<br><br>\n",
    "The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted return:\n",
    "<img src='fig9.png' width='450'>\n",
    "<img src='fig10.png' width='400'>\n",
    "Note that this works for all time steps $t<T$, even if termination occurs at $t + 1$, if we define $G_T = 0$. This often makes it easy to compute returns from reward sequences.<br><br>\n",
    "Although in continuing tasks return is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant—if $\\gamma < 1$. For example, if the reward is a constant +1, then the return is:\n",
    "<img src='fig11.png' width='150'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "<img src='fig12.png' width='700'>\n",
    "<img src='fig13.png' width='700'>\n",
    "<img src='fig14.png' width='700'>\n",
    "<img src='fig15.png' width='700'>\n",
    "<img src='fig16.png' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "#### Agent & Environment\n",
    "An agent that controls 11 players (robots) of a team in a robot soccer game. \n",
    "#### State\n",
    "The state can be defined by current position of football, position & velocity of 11 players of opponent team, position & velocity of 11 players of the agent's team, position of goal post. \n",
    "#### Actions\n",
    "Actions include all possible actions for each of the 11 players viz. running, tackling, kicking with all possible speeds and direction.\n",
    "#### Rewards\n",
    "+1 reward for kicking the ball into opponent's goal post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://mitpress.mit.edu/books/reinforcement-learning<br>\n",
    "https://www.coursera.org/specializations/reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
